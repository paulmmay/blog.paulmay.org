---
layout: post
title: Putting Your Face in a Webpage
category: Appropriating New Technologies
comments: true
---

**FaceDOM uses Kyle McDonald's [ofxFaceTracker](https://github.com/kylemcdonald/ofxFaceTracker) to send the points of a person's face to a web page and move DOM elements into the shape of the face.**

<iframe src="http://player.vimeo.com/video/36563425" width="772" height="434" frameborder="0"></iframe>

This past week in Kyle's [Appropriating New Technologies](http://github.com/kylemcdonald/AppropriatingNewTechnologies) we were asked to do something with face detection. After seeing Greg Borenstein send the Kinect skeleton data to a web browser [last fall](http://urbanhonking.com/ideasfordozens/2011/10/27/streaming-kinect-skeleton-data-to-the-web-with-node-js/) I've wanted to experiment with Node.js and sending things to the browser that don't traditionally belong there.

Also this project is in response to Kyle's [People Staring At Computers](https://vimeo.com/groups/openframeworks/videos/25958231). We stare at our computers a lot and they don't respond to our stares. Mouse clicks and key hits yes, but reading an article on the web is still largely non-interactive.

Currently FaceDOM is a proof of concept. I plan to add more features and a finer amount of detail and response to facial gestures, such as an open mouth, closed eyes, etc. Another important addition will be dynamically parsing the page, currently the elements chosen to form my face were directly coded with the proper CSS tags.

The source code is available on Github [here](https://github.com/stevenklise/AppropriatingNewTechnologies/tree/master/week2).