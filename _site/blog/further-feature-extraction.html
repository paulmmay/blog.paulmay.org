<!DOCTYPE html>
<html lang="en"> 
	<head>
		<meta charset="utf-8">
		<title>More Audio Feature Extraction | Paul May</title>
		<meta name="description" content="">
		<meta name="keywords" content="">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes"/>
		<meta name="generator" content="">

		<link rel="stylesheet" type="text/css" href="/s/r.css" media="all">
		<link rel="stylesheet" type="text/css" href="/s/s.css" media="all"/>
		<link href="https://fonts.googleapis.com/css?family=Alegreya" rel="stylesheet">		

	</head>

	<body>

	<header role="banner">
		<ul class="nav main">
<li><a href="/about/">About</a></li>

<li><a href="/blog/" class="visited">Blog</a></li>

<li><a href="/work/">Work</a></li>

</ul>
	</header>

	<!--The top slice of the article; author, title, date and other cruft
	I still want it to break left right like the main content div.
	Two containers? Yes. Two bloody containers. Deal with it. -->
	<div class="wrapper top">
		<div class="left masthead">
			<p class="author">Paul May</p>
			<h1><a href="/blog/further-feature-extraction">More Audio Feature Extraction</a></h1>
			<p class="datetime">07 August 2016</p>
		</div>
	</div>

	<!--The content slice of the article. On the left we have the article text,
		then on the right we have the little monogram and any other cruft we 
		care to include. And when I say we, I do of course mean I.-->
	<div class="wrapper main">
		<div class="left content">
		<p>I’m making incremental progress on <a href="/blog/audio-information-extraction">my listening robot project</a>. The ultimate goal is to build a little machine capable of listening to its surroundings, learning from the sound it hears, and then - later - being able to determine its location based upon what it hears. A reasonable prototype robot might be able to remember 5-10 locations, and then tell those locations apart when asked to do so.</p>

<p>There are a number of ways of approaching the software part of the project. I’ve chosen to go down a classical, supervised machine learning route - extracting features from sound, then using them to fit a machine learning model. In essence, this means taking raw sound, and <em>noticing</em> particular qualities of the sound. These qualities are associated with a label - say the name of the location - and used to train a machine learning model.</p>

<p>My work in the last week has been to implement simple feature extraction techniques demonstrated in off-the-shelf audio information extraction libraries Librosa and pyAudioAnalysis. I have been passing in short snippets of music into their feature extraction functions, to output specific features of that sound - <a href="&quot;https://en.wikipedia.org/wiki/Spectral_centroid&quot;">spectral centroid</a>; the “brightness of a sound”, <a href="&quot;http://www.ese.wustl.edu/~nehorai/paper/specom03.pdf&quot;">spectral contrast</a>; the difference between peaks and valleys in a sound.</p>

<p>In software, I’m implementing litte feature extractor classes that can be passed some sound, returning feature representations of the sound. I’m trying to get the structure of this right, up front, so I don’t have a lot of refactoring and noodling to do later. I’m also trying to be somewhat agnostic of the underlying library, so that they can be mixed/swapped as needs be.</p>

<p>I’m getting close to just piping feature data into some dumb classifier; getting to a crude proof of concept. I’m not sure whether information extraction techniques more geared towards understanding <em>music</em> are useful for understanding ambient sound - if they’re not, then I may need to adopt another approach.</p>

		
<div class="project-section">
<h3>Related Images</h3>


<figure>
<a href="https://c1.staticflickr.com/9/8607/28826205035_b247fd841a_o.png"><img src="https://c1.staticflickr.com/9/8607/28826205035_b247fd841a_o.png" class="photo"/></a>
<figcaption>Rihanna track - The spectral centroid fluctuates in line with the beat, peaking on the snare drum/white noise</figcaption>
</figure>


<figure>
<a href="https://c1.staticflickr.com/9/8773/28721275462_cf07d7f7a2_o.png"><img src="https://c1.staticflickr.com/9/8773/28721275462_cf07d7f7a2_o.png" class="photo"/></a>
<figcaption>Rihanna track - The spectral contrast of the track. I'm not sure if this a useful representation of the sound - more reading to do</figcaption>
</figure>


<figure>
<a href="https://c1.staticflickr.com/9/8681/28721275742_ab5f457813_o.png"><img src="https://c1.staticflickr.com/9/8681/28721275742_ab5f457813_o.png" class="photo"/></a>
<figcaption>Mozart - The spectral centroid is much more even, consistent than the Rihanna track. The brightness of the sound tracks the notes played on the piano, peaking on the highest and brightest note.</figcaption>
</figure>


<figure>
<a href="https://c1.staticflickr.com/9/8832/28721275892_88e972e74a_o.png"><img src="https://c1.staticflickr.com/9/8832/28721275892_88e972e74a_o.png" class="photo"/></a>
<figcaption>Mozart - The spectral contrast of the track. Clearly different from the Rihanna track, but a useful way to represent the sound? I'm not so sure.</figcaption>
</figure>

</div>

		
<div class="project-section">
<h3>Source Code &amp; References</h3>
<ul>



<li><a href="https://en.wikipedia.org/wiki/Spectral_centroid">Spectral Centroid definition</a></li>


<li><a href="http://www.ese.wustl.edu/~nehorai/paper/specom03.pdf">Spectral contrast enhancement - Algorithms and comparisons</a></li>


<li><a href="http://www.ncbi.nlm.nih.gov/pubmed/26936556">Spectral contrast enhancement improves speech intelligibility in noise for cochlear implants.</a></li>


<li><a href="http://librosa.github.io/librosa/index.html">LibRosa library for Python</a></li>


<li><a href="https://github.com/tyiannak/pyAudioAnalysis">pyAudioAnalysis library for Python</a></li>


</div>

	
	

	
	<div id="templates">
	<div id="tweet_template">
		<li class="tweet"><span class="text">{{text}}</span><div class="tweetdate">{{date}}</div></li>
	</div>
	</div>
	

	<div class="pagination">
	
	 
	
	<a href="/blog/audio-information-extraction" class="next-link" title="">Older Article &raquo;</a>
	
	</div>


		</div>
		<!--The afforementioned monogram and cruft-->
		<div class="right minibio">
		Paul May is a researcher, interaction designer, and technologist from Dublin, Ireland. He is currently <a href="/work/">working with Memorial Sloan Kettering Cancer Center</a> on smart health applications.	
<div class="elsewhere">
<ul>
<li><a href="https://twitter.com/paulmmay">@paulmmay</a></li>
<li><a href="mailto:hello@paulmay.org">hello@paulmay.org</a></li>
</ul>
</div>

	

		</div>
	</div>

	<footer>
		<p>&copy; Paul May.</p>
		<ul class="footernav">
		</ul>
	</footer>



</body>
</html>
